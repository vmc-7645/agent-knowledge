import os
import sys
import argparse
import asyncio
import weave

# --------------------------------------------------
# ENVIRONMENT
# --------------------------------------------------
from dotenv import load_dotenv
load_dotenv()

# --------------------------------------------------
# OPENAI
# --------------------------------------------------
from openai import OpenAI
client = OpenAI()

# --------------------------------------------------
# MEMORY
# --------------------------------------------------
# Import async memory helpers generated by the user
import memory  # assumes memory.py lives in agent/ __init__.py not required

# --------------------------------------------------
# WEAVE
# --------------------------------------------------
WEAVE_PROJECT = os.getenv("WEAVE_PROJECT", "wandbhackathon")

# CLI args
parser = argparse.ArgumentParser()
parser.add_argument("--persona", type=str, default="your helpful assistant")
args = parser.parse_args()
persona = args.persona

# Initialise Weave
weave.init(WEAVE_PROJECT)

# --------------------------------------------------
# LLM Agent â€“ single op
# --------------------------------------------------
@weave.op()
def ask_llm(prompt: str, persona: str) -> str:
    """Single GPT-4 call wrapped for observability with persona injection."""
    full_prompt = f"Answer the following as if you were {persona}, keep your answer under 5 sentences long and concise:\n\n{prompt}"
    
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": full_prompt}],
    )
    return response.choices[0].message.content.strip()


# --------------------------------------------------
# CLI LOOP
# --------------------------------------------------
async def chat():
    print(
        f"""
    Persona: {persona}
    Model: gpt-4o-mini
    ---
    Type 'exit' to quit.
    """
    )
    while True:
        try:
            user_input = input("You > ")
        except (KeyboardInterrupt, EOFError):
            print()
            break

        if user_input.lower() in {"exit", "quit"}:
            break

        # Weave-tracked LLM call
        answer = ask_llm(user_input, persona)
        print(f"Agent > {answer}\n")

        # Persist memory (embedding handled server-side -> pass [])
        try:
            await asyncio.gather(
                memory.remember_question(user_input, []),
                memory.remember_answer(answer, []),
            )
        except Exception as e:  # best-effort storage
            print(f"[memory-warning] {e}", file=sys.stderr)


if __name__ == "__main__":
    asyncio.run(chat())
